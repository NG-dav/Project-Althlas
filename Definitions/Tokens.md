### Tokens (in AI/LLMs): The Core Concept

In the context of AI, particularly Large Language Models (LLMs), **Tokens** are the **fundamental units of text** that the model processes. Think of them as pieces of words, whole words, or even punctuation marks and characters that text is broken down into before the AI can understand or generate language[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKC9YCYUdgQ9yAKUElZM9lSbg5jo_kuS3NbOwqwv-qSM8Vdw9eeenqzAY6TyV_YvEDd87XptowuB0LaUTzV7bZ5eAQzlw8rCNYg86kMM6q2lSiazstelrjm270p9detG4gTbvRAt41S7CaPnqYwKPbHouPwHNHSOInvfW8%3D)][[2](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALHQ0UtaD0Bg4VJcV6tK62NWr-E8l-iUvqOBFHglMWAM19S-UOxitFdpbBi4Y28dUA4mB1kn-Y37S8MVKxiqsCTw7eYgw5I9aFrEatNrSzhUWxkskjCJyXGdNMHOksdrfuTGpMWaMGR6DPxw0JMG2E1NkQ%3D)]. Models don't see raw text; they see sequences of these tokens, which are then typically converted into numbers[[3](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ8RbvbWgs5gs6G3-zif5M_m3Wc_Qo3Sfgbkg0BOBVOPpoxfu-8CcOMUrlgYRsm6nwl5SQn-99lvhwx0c_XyQ283NRXbuYRPWJFilM2c78LrgJ7zI-H641hfNE1wHSC8yz92MP6uBuEbObyoqRqiNqwHQXnZ1isitVWK9qfOP0Kjuqu)][[4](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAI5hMEo13Rb50ZmUODC5MzEYm1tgVdz5bqJzVbviTcDYsIqlVHOfRxSI89-DKlYlC4colIFNxIKcc4qMRysfN7yCyUZGbVG3d_xlunQ3KgZiQZcoVLjB9WMUHNO2cpkb-ld5WhijRujM4Fb)]. **In essence: Tokens are the building blocks, like Lego bricks, that AI models use to read, understand, and write text.**

---

### In-Depth Information on Tokens

Let's explore the details of tokens and tokenization:

1. **What is Tokenization?**
    
    - Tokenization is the process of converting raw text into a sequence of tokens[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKC9YCYUdgQ9yAKUElZM9lSbg5jo_kuS3NbOwqwv-qSM8Vdw9eeenqzAY6TyV_YvEDd87XptowuB0LaUTzV7bZ5eAQzlw8rCNYg86kMM6q2lSiazstelrjm270p9detG4gTbvRAt41S7CaPnqYwKPbHouPwHNHSOInvfW8%3D)][[2](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALHQ0UtaD0Bg4VJcV6tK62NWr-E8l-iUvqOBFHglMWAM19S-UOxitFdpbBi4Y28dUA4mB1kn-Y37S8MVKxiqsCTw7eYgw5I9aFrEatNrSzhUWxkskjCJyXGdNMHOksdrfuTGpMWaMGR6DPxw0JMG2E1NkQ%3D)][[5](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKr-l8HI8_LusObg8KlVBLRgdLDURGTsPs0qFZsgNCUQtcaVMkKTTeQtlb9TludjaL13mJZiVmIt7fkGeADZpvhwtct3wirs9_CV47rdFqbIwELHU5RSYbUEvIgpOFw0qKC)]. It's a necessary first step because AI models, especially neural networks, operate on numerical data, not raw strings of characters[[3](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ8RbvbWgs5gs6G3-zif5M_m3Wc_Qo3Sfgbkg0BOBVOPpoxfu-8CcOMUrlgYRsm6nwl5SQn-99lvhwx0c_XyQ283NRXbuYRPWJFilM2c78LrgJ7zI-H641hfNE1wHSC8yz92MP6uBuEbObyoqRqiNqwHQXnZ1isitVWK9qfOP0Kjuqu)][[4](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAI5hMEo13Rb50ZmUODC5MzEYm1tgVdz5bqJzVbviTcDYsIqlVHOfRxSI89-DKlYlC4colIFNxIKcc4qMRysfN7yCyUZGbVG3d_xlunQ3KgZiQZcoVLjB9WMUHNO2cpkb-ld5WhijRujM4Fb)].
        
    - A program or algorithm called a "tokenizer" performs this process[[3](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ8RbvbWgs5gs6G3-zif5M_m3Wc_Qo3Sfgbkg0BOBVOPpoxfu-8CcOMUrlgYRsm6nwl5SQn-99lvhwx0c_XyQ283NRXbuYRPWJFilM2c78LrgJ7zI-H641hfNE1wHSC8yz92MP6uBuEbObyoqRqiNqwHQXnZ1isitVWK9qfOP0Kjuqu)]. Each tokenizer has a predefined "vocabulary" – the set of all possible unique tokens it knows[[6](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKuAnGymumgOocypBSZvqdxtv8s_m8ZyXGntPcwvNLvcIrXxnBbTgAJsPG9dz94jfty8Ix-3tEyVma_LSPZgzFkYNAtKgY27bwY5FsxnWRtocwmvfspaC6MUbX-gxZ-6fTh55erxMg4w5JimovIdfbBikjwcAVT085hApDS1LmU8Q%3D%3D)].
        
2. **Why Use Tokens Instead of Words or Characters?**
    
    - **Handling Complexity:** Using tokens, especially subword tokens (parts of words), allows models to handle complex language, including new or rare words, typos, and different word forms, by breaking them down into known smaller pieces[[3](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ8RbvbWgs5gs6G3-zif5M_m3Wc_Qo3Sfgbkg0BOBVOPpoxfu-8CcOMUrlgYRsm6nwl5SQn-99lvhwx0c_XyQ283NRXbuYRPWJFilM2c78LrgJ7zI-H641hfNE1wHSC8yz92MP6uBuEbObyoqRqiNqwHQXnZ1isitVWK9qfOP0Kjuqu)][[6](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKuAnGymumgOocypBSZvqdxtv8s_m8ZyXGntPcwvNLvcIrXxnBbTgAJsPG9dz94jfty8Ix-3tEyVma_LSPZgzFkYNAtKgY27bwY5FsxnWRtocwmvfspaC6MUbX-gxZ-6fTh55erxMg4w5JimovIdfbBikjwcAVT085hApDS1LmU8Q%3D%3D)]. This is more flexible than strict word-based tokenization, which struggles with unknown words, or character-based tokenization, which can create very long sequences[[4](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAI5hMEo13Rb50ZmUODC5MzEYm1tgVdz5bqJzVbviTcDYsIqlVHOfRxSI89-DKlYlC4colIFNxIKcc4qMRysfN7yCyUZGbVG3d_xlunQ3KgZiQZcoVLjB9WMUHNO2cpkb-ld5WhijRujM4Fb)][[7](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ5XrzC21b_Ima0ob00QZGCvc_H3-MTydUhZ2IV4DhnSsgSdM_8gb4bVLfxww3EICZXf0Fx0nrK47Zo3SMJA4eB0-XPsF_q5Khni0X3WsJv_0m8dN-39_ClOIX7NaWCelbrbXi20KQDrd6Vb5WbcKNFjM-7GK9UmGA5ubXZR_B9xu2la2NIKrJVZsUZrA%3D%3D)].
        
    - **Efficiency:** Models process tokens to learn relationships and patterns[[6](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKuAnGymumgOocypBSZvqdxtv8s_m8ZyXGntPcwvNLvcIrXxnBbTgAJsPG9dz94jfty8Ix-3tEyVma_LSPZgzFkYNAtKgY27bwY5FsxnWRtocwmvfspaC6MUbX-gxZ-6fTh55erxMg4w5JimovIdfbBikjwcAVT085hApDS1LmU8Q%3D%3D)][[8](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKIicIayDyxq5MGRpl8cYde5FzbY_39DEGaaWag6QSH9P5ZAmzpQkN77FzSYuA95panhiQNLXVzSKr1LGzXuZrGtoXrU4651rsp3-qNSzOTrQJ0ED5R9thhg3whYpVYBzpMeciQJG0Z)]. Subword tokenization often provides a good balance between vocabulary size (how many unique tokens the model needs to know) and sequence length (how many tokens represent a piece of text)[[2](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALHQ0UtaD0Bg4VJcV6tK62NWr-E8l-iUvqOBFHglMWAM19S-UOxitFdpbBi4Y28dUA4mB1kn-Y37S8MVKxiqsCTw7eYgw5I9aFrEatNrSzhUWxkskjCJyXGdNMHOksdrfuTGpMWaMGR6DPxw0JMG2E1NkQ%3D)]. Word-based systems can have huge vocabularies, while character-based systems create very long sequences, increasing computational load[[4](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAI5hMEo13Rb50ZmUODC5MzEYm1tgVdz5bqJzVbviTcDYsIqlVHOfRxSI89-DKlYlC4colIFNxIKcc4qMRysfN7yCyUZGbVG3d_xlunQ3KgZiQZcoVLjB9WMUHNO2cpkb-ld5WhijRujM4Fb)][[7](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ5XrzC21b_Ima0ob00QZGCvc_H3-MTydUhZ2IV4DhnSsgSdM_8gb4bVLfxww3EICZXf0Fx0nrK47Zo3SMJA4eB0-XPsF_q5Khni0X3WsJv_0m8dN-39_ClOIX7NaWCelbrbXi20KQDrd6Vb5WbcKNFjM-7GK9UmGA5ubXZR_B9xu2la2NIKrJVZsUZrA%3D%3D)].
        
    - **Numerical Representation:** Ultimately, tokenization allows text to be converted into sequences of numbers (token IDs), which can then be transformed into embeddings (vector representations) that capture semantic meaning for the model to process[[3](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ8RbvbWgs5gs6G3-zif5M_m3Wc_Qo3Sfgbkg0BOBVOPpoxfu-8CcOMUrlgYRsm6nwl5SQn-99lvhwx0c_XyQ283NRXbuYRPWJFilM2c78LrgJ7zI-H641hfNE1wHSC8yz92MP6uBuEbObyoqRqiNqwHQXnZ1isitVWK9qfOP0Kjuqu)][[6](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKuAnGymumgOocypBSZvqdxtv8s_m8ZyXGntPcwvNLvcIrXxnBbTgAJsPG9dz94jfty8Ix-3tEyVma_LSPZgzFkYNAtKgY27bwY5FsxnWRtocwmvfspaC6MUbX-gxZ-6fTh55erxMg4w5JimovIdfbBikjwcAVT085hApDS1LmU8Q%3D%3D)][[9](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ433CD791L7XBqwf-kcR4dxxaeBScCDBn82GIv1XOhyxXy6MSIS211I60AD2sKEtPvqX26DtkEVvITqVysvNuNhjM2nlM1Nd2gVBqLb6bg14RtkeHkQgMevZuexn21yrrnbtRVMFBJ4F3PUXeNtWul8i4XEw%3D%3D)].
        
3. **Common Tokenization Methods:**
    
    - **Word Tokenization:** Splits text based on spaces and punctuation[[7](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ5XrzC21b_Ima0ob00QZGCvc_H3-MTydUhZ2IV4DhnSsgSdM_8gb4bVLfxww3EICZXf0Fx0nrK47Zo3SMJA4eB0-XPsF_q5Khni0X3WsJv_0m8dN-39_ClOIX7NaWCelbrbXi20KQDrd6Vb5WbcKNFjM-7GK9UmGA5ubXZR_B9xu2la2NIKrJVZsUZrA%3D%3D)][[10](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ5bC7c8r_aul4Qln9EWZFYoFeDlRrpAhopmlx9KmKgKH6bg62cPs8HG6ms2YlD6JDlAbAs__hLYs_Bu5SsA0sWamIL5g2KcWpwAP1tx5cRUZCbK_UbzShd43DjEcDIM3_v563i9kizSVX8_E_uq_6e)]. Simple but struggles with contractions, unknown words, and languages without clear spaces[[7](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ5XrzC21b_Ima0ob00QZGCvc_H3-MTydUhZ2IV4DhnSsgSdM_8gb4bVLfxww3EICZXf0Fx0nrK47Zo3SMJA4eB0-XPsF_q5Khni0X3WsJv_0m8dN-39_ClOIX7NaWCelbrbXi20KQDrd6Vb5WbcKNFjM-7GK9UmGA5ubXZR_B9xu2la2NIKrJVZsUZrA%3D%3D)][[11](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALes-FzTNwvqWW8g5keFYUF3E9u4uLDESZtaWi63_nEj5_TJVInyiBHekjRX0atg4B2XwtPE6VHrLj-dNFMfT8p_6Ui-YNhZ4xykILl41xaBCknMwMHEH5qxUjAUNkgVbmpXVY2Ah-6IDN-pWYcS2fm0RcSxvJN04WohyWMX7cOCAMdoCyqwsibQ4Q9nBI%3D)].
        
    - **Character Tokenization:** Splits text into individual characters[[6](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKuAnGymumgOocypBSZvqdxtv8s_m8ZyXGntPcwvNLvcIrXxnBbTgAJsPG9dz94jfty8Ix-3tEyVma_LSPZgzFkYNAtKgY27bwY5FsxnWRtocwmvfspaC6MUbX-gxZ-6fTh55erxMg4w5JimovIdfbBikjwcAVT085hApDS1LmU8Q%3D%3D)][[7](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ5XrzC21b_Ima0ob00QZGCvc_H3-MTydUhZ2IV4DhnSsgSdM_8gb4bVLfxww3EICZXf0Fx0nrK47Zo3SMJA4eB0-XPsF_q5Khni0X3WsJv_0m8dN-39_ClOIX7NaWCelbrbXi20KQDrd6Vb5WbcKNFjM-7GK9UmGA5ubXZR_B9xu2la2NIKrJVZsUZrA%3D%3D)]. Handles any word but creates long sequences and loses the inherent meaning of words[[7](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ5XrzC21b_Ima0ob00QZGCvc_H3-MTydUhZ2IV4DhnSsgSdM_8gb4bVLfxww3EICZXf0Fx0nrK47Zo3SMJA4eB0-XPsF_q5Khni0X3WsJv_0m8dN-39_ClOIX7NaWCelbrbXi20KQDrd6Vb5WbcKNFjM-7GK9UmGA5ubXZR_B9xu2la2NIKrJVZsUZrA%3D%3D)].
        
    - **Subword Tokenization:** The most common approach in modern LLMs. It breaks words into smaller, meaningful units. Frequent words might remain whole tokens, while rare words are broken down[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKC9YCYUdgQ9yAKUElZM9lSbg5jo_kuS3NbOwqwv-qSM8Vdw9eeenqzAY6TyV_YvEDd87XptowuB0LaUTzV7bZ5eAQzlw8rCNYg86kMM6q2lSiazstelrjm270p9detG4gTbvRAt41S7CaPnqYwKPbHouPwHNHSOInvfW8%3D)]. This balances vocabulary size and sequence length effectively[[6](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKuAnGymumgOocypBSZvqdxtv8s_m8ZyXGntPcwvNLvcIrXxnBbTgAJsPG9dz94jfty8Ix-3tEyVma_LSPZgzFkYNAtKgY27bwY5FsxnWRtocwmvfspaC6MUbX-gxZ-6fTh55erxMg4w5JimovIdfbBikjwcAVT085hApDS1LmU8Q%3D%3D)]. Common subword algorithms include:
        
        - **Byte-Pair Encoding (BPE):** Starts with individual characters (or bytes) and iteratively merges the most frequent pairs to create new subword tokens[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKC9YCYUdgQ9yAKUElZM9lSbg5jo_kuS3NbOwqwv-qSM8Vdw9eeenqzAY6TyV_YvEDd87XptowuB0LaUTzV7bZ5eAQzlw8rCNYg86kMM6q2lSiazstelrjm270p9detG4gTbvRAt41S7CaPnqYwKPbHouPwHNHSOInvfW8%3D)][[5](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKr-l8HI8_LusObg8KlVBLRgdLDURGTsPs0qFZsgNCUQtcaVMkKTTeQtlb9TludjaL13mJZiVmIt7fkGeADZpvhwtct3wirs9_CV47rdFqbIwELHU5RSYbUEvIgpOFw0qKC)][[12](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAIJn1406nFi_hinn5mVMsvSH5WGlQ0O0wsj9twsdS2MFTb163wfALSRtDQI9X0kX8Vyy9EyDWC85fevBKFqPiZ8FwnxViZuZEBsh691qu-8llV3Vo8ZUbvKLeo%3D)]. Used by models like GPT-2, GPT-3, and Llama[[12](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAIJn1406nFi_hinn5mVMsvSH5WGlQ0O0wsj9twsdS2MFTb163wfALSRtDQI9X0kX8Vyy9EyDWC85fevBKFqPiZ8FwnxViZuZEBsh691qu-8llV3Vo8ZUbvKLeo%3D)][[13](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALUF9n7ICff6mRnjQjGaCQOTrq9X9IOsrHvF9JcNsmom8aVC-V63sE4rAsa9-xhhcresRhnrVfiGwrmkd-QtMKpcai7puI-vYm1zEy7n9FOmDccHA1KBZ2fK4x3Gld5S4yjPzd6zKwNJosLkuLNwk0JucsfSmE%3D)]. Byte-level BPE ensures any character can be represented[[13](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALUF9n7ICff6mRnjQjGaCQOTrq9X9IOsrHvF9JcNsmom8aVC-V63sE4rAsa9-xhhcresRhnrVfiGwrmkd-QtMKpcai7puI-vYm1zEy7n9FOmDccHA1KBZ2fK4x3Gld5S4yjPzd6zKwNJosLkuLNwk0JucsfSmE%3D)].
            
        - **WordPiece:** Similar to BPE, but merges pairs based on which merge maximizes the likelihood of the training data, rather than just frequency[[11](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALes-FzTNwvqWW8g5keFYUF3E9u4uLDESZtaWi63_nEj5_TJVInyiBHekjRX0atg4B2XwtPE6VHrLj-dNFMfT8p_6Ui-YNhZ4xykILl41xaBCknMwMHEH5qxUjAUNkgVbmpXVY2Ah-6IDN-pWYcS2fm0RcSxvJN04WohyWMX7cOCAMdoCyqwsibQ4Q9nBI%3D)][[12](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAIJn1406nFi_hinn5mVMsvSH5WGlQ0O0wsj9twsdS2MFTb163wfALSRtDQI9X0kX8Vyy9EyDWC85fevBKFqPiZ8FwnxViZuZEBsh691qu-8llV3Vo8ZUbvKLeo%3D)][[14](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAK9CmfKiLEMWkCSy4i_S-GrS1wO8jJyGG6KPK18LoKI2YRRxBBXuLscYdRWJ8HZFNRgkGgTFvPylat5xJbYpEhUDlHdON41iLT-_EPB67CG9L7Nb_tQiTZr9H__84GpfzhgCTgF9f-Bpg%3D%3D)]. Used by BERT[[11](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALes-FzTNwvqWW8g5keFYUF3E9u4uLDESZtaWi63_nEj5_TJVInyiBHekjRX0atg4B2XwtPE6VHrLj-dNFMfT8p_6Ui-YNhZ4xykILl41xaBCknMwMHEH5qxUjAUNkgVbmpXVY2Ah-6IDN-pWYcS2fm0RcSxvJN04WohyWMX7cOCAMdoCyqwsibQ4Q9nBI%3D)][[13](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALUF9n7ICff6mRnjQjGaCQOTrq9X9IOsrHvF9JcNsmom8aVC-V63sE4rAsa9-xhhcresRhnrVfiGwrmkd-QtMKpcai7puI-vYm1zEy7n9FOmDccHA1KBZ2fK4x3Gld5S4yjPzd6zKwNJosLkuLNwk0JucsfSmE%3D)].
            
        - **SentencePiece:** Treats text as a raw sequence, including spaces, and uses BPE or Unigram methods. Good for multilingual models as it doesn't assume space delimiters[[11](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALes-FzTNwvqWW8g5keFYUF3E9u4uLDESZtaWi63_nEj5_TJVInyiBHekjRX0atg4B2XwtPE6VHrLj-dNFMfT8p_6Ui-YNhZ4xykILl41xaBCknMwMHEH5qxUjAUNkgVbmpXVY2Ah-6IDN-pWYcS2fm0RcSxvJN04WohyWMX7cOCAMdoCyqwsibQ4Q9nBI%3D)][[12](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAIJn1406nFi_hinn5mVMsvSH5WGlQ0O0wsj9twsdS2MFTb163wfALSRtDQI9X0kX8Vyy9EyDWC85fevBKFqPiZ8FwnxViZuZEBsh691qu-8llV3Vo8ZUbvKLeo%3D)][[15](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKvH8rMIeILRSUr5ufjLeZ4x5JfkVspElNCtH7s8bhhAPkeTi2uk9DyJYQcrlZkolwwS4WLuujRVEc3GIzffA6yN88DNq41K-ieHet1dukx2t8BhhKVeJaT3apyJk7q)]. Used in models like ALBERT and T5[[12](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAIJn1406nFi_hinn5mVMsvSH5WGlQ0O0wsj9twsdS2MFTb163wfALSRtDQI9X0kX8Vyy9EyDWC85fevBKFqPiZ8FwnxViZuZEBsh691qu-8llV3Vo8ZUbvKLeo%3D)][[15](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKvH8rMIeILRSUr5ufjLeZ4x5JfkVspElNCtH7s8bhhAPkeTi2uk9DyJYQcrlZkolwwS4WLuujRVEc3GIzffA6yN88DNq41K-ieHet1dukx2t8BhhKVeJaT3apyJk7q)].
            
4. **Significance of Tokens:**
    
    - **Model Input/Output Limits (Context Window):** LLMs have a maximum limit on the number of tokens they can process in a single input or generate in a single output, known as the "context window"[[8](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKIicIayDyxq5MGRpl8cYde5FzbY_39DEGaaWag6QSH9P5ZAmzpQkN77FzSYuA95panhiQNLXVzSKr1LGzXuZrGtoXrU4651rsp3-qNSzOTrQJ0ED5R9thhg3whYpVYBzpMeciQJG0Z)][[16](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKpuZ7UG65VgbFJiEn3oMxkehcB4Mfhupk5SIicepT920oVaTVt8vgTLyQXZMZavKi0ePGmL8WzFyFb1uJRX4Arn7Ngz3QkiUtd34xJDUIVeVydo2cQbCcNJLKWj20Mbl3VQ6mhvu3nayh96FPdeiu75gYjFfKd-0KNiXZQY9L_TUpEzA%3D%3D)][[17](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKHxMTXeQS0HRHVoTYOOjaoCPuZbnBN0rXEci4GvFH1cNzolhWhDhL9CNctZqeU2yXvab8vDunstz9ReI3hTZcP_uq-nUx_5z7o3fb_Ij04KEt-TVBJSrdo5mrGqY9TpTunLQ%3D%3D)]. This limit (e.g., 4096, 8192, 128,000 tokens) affects how much text or how long a conversation the model can handle at once[[17](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKHxMTXeQS0HRHVoTYOOjaoCPuZbnBN0rXEci4GvFH1cNzolhWhDhL9CNctZqeU2yXvab8vDunstz9ReI3hTZcP_uq-nUx_5z7o3fb_Ij04KEt-TVBJSrdo5mrGqY9TpTunLQ%3D%3D)][[18](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKAuhn_c1c8ETXuLOrNe-yGhaDrZ_8aUeyEmaygDlXT1BZmfn7NYJDhUAwY_iEFSsgMRfRIYyzXfF8Le9ZtRNcNozK7crplSInymKO1D_TPqg94jgKk-h0JjPWZN-nGk9P8)].
        
    - **Cost:** Many LLM APIs charge based on the number of tokens processed (both input tokens and output tokens)[[5](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKr-l8HI8_LusObg8KlVBLRgdLDURGTsPs0qFZsgNCUQtcaVMkKTTeQtlb9TludjaL13mJZiVmIt7fkGeADZpvhwtct3wirs9_CV47rdFqbIwELHU5RSYbUEvIgpOFw0qKC)][[17](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKHxMTXeQS0HRHVoTYOOjaoCPuZbnBN0rXEci4GvFH1cNzolhWhDhL9CNctZqeU2yXvab8vDunstz9ReI3hTZcP_uq-nUx_5z7o3fb_Ij04KEt-TVBJSrdo5mrGqY9TpTunLQ%3D%3D)][[19](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALQJ71nvi7Yu281Xe4-CS6O1vjLWd08Kix4FVLEGiAhE05H--vxAMvG6AoyBv4E6_bicKdR1Ul8Eeeymty0ZQEWrCVWMbHq4w6kYjp-BBFwbmxsNrAsUF-FEiu-OWT6HFCeUqXevixDpo5JCxw3VrA12Vkg2BtzP7xpsbJq0zVAHXVAZxdF1hA8CEY%3D)]. Therefore, understanding token count is crucial for managing costs[[16](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKpuZ7UG65VgbFJiEn3oMxkehcB4Mfhupk5SIicepT920oVaTVt8vgTLyQXZMZavKi0ePGmL8WzFyFb1uJRX4Arn7Ngz3QkiUtd34xJDUIVeVydo2cQbCcNJLKWj20Mbl3VQ6mhvu3nayh96FPdeiu75gYjFfKd-0KNiXZQY9L_TUpEzA%3D%3D)][[17](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKHxMTXeQS0HRHVoTYOOjaoCPuZbnBN0rXEci4GvFH1cNzolhWhDhL9CNctZqeU2yXvab8vDunstz9ReI3hTZcP_uq-nUx_5z7o3fb_Ij04KEt-TVBJSrdo5mrGqY9TpTunLQ%3D%3D)][[20](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALP7xmZCVCnEmS0knz2WtJp1Sm8lclSpsRgNW1kZgTn0hdcbAmpK1JusaAMZ2Q45IN_8yLzL_mL8aCDT_wTe3Yi7CF4ZoQjciECW27GUsM6ZUyrheqQJuv2che6kn_QTllhpaCEpe53zPPgjgHKwbULbkjlBZo%3D)]. As a rough guide, for English text, one token often corresponds to about 4 characters or roughly ¾ of a word[[16](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKpuZ7UG65VgbFJiEn3oMxkehcB4Mfhupk5SIicepT920oVaTVt8vgTLyQXZMZavKi0ePGmL8WzFyFb1uJRX4Arn7Ngz3QkiUtd34xJDUIVeVydo2cQbCcNJLKWj20Mbl3VQ6mhvu3nayh96FPdeiu75gYjFfKd-0KNiXZQY9L_TUpEzA%3D%3D)].
        
    - **Performance and Behavior:** The way text is tokenized can influence model performance, sometimes leading to unexpected behavior like difficulty with spelling, simple arithmetic, or certain languages[[2](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALHQ0UtaD0Bg4VJcV6tK62NWr-E8l-iUvqOBFHglMWAM19S-UOxitFdpbBi4Y28dUA4mB1kn-Y37S8MVKxiqsCTw7eYgw5I9aFrEatNrSzhUWxkskjCJyXGdNMHOksdrfuTGpMWaMGR6DPxw0JMG2E1NkQ%3D)]. Efficient tokenization can reduce computational load and latency[[8](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKIicIayDyxq5MGRpl8cYde5FzbY_39DEGaaWag6QSH9P5ZAmzpQkN77FzSYuA95panhiQNLXVzSKr1LGzXuZrGtoXrU4651rsp3-qNSzOTrQJ0ED5R9thhg3whYpVYBzpMeciQJG0Z)][[17](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKHxMTXeQS0HRHVoTYOOjaoCPuZbnBN0rXEci4GvFH1cNzolhWhDhL9CNctZqeU2yXvab8vDunstz9ReI3hTZcP_uq-nUx_5z7o3fb_Ij04KEt-TVBJSrdo5mrGqY9TpTunLQ%3D%3D)].
        
5. **Example:**
    
    - Text: Tokenization is important!
        
    - Word Tokens: ["Tokenization", "is", "important!"][[13](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALUF9n7ICff6mRnjQjGaCQOTrq9X9IOsrHvF9JcNsmom8aVC-V63sE4rAsa9-xhhcresRhnrVfiGwrmkd-QtMKpcai7puI-vYm1zEy7n9FOmDccHA1KBZ2fK4x3Gld5S4yjPzd6zKwNJosLkuLNwk0JucsfSmE%3D)].
        
    - Subword Tokens (example): ["Token", "ization", "is", "important", "!"].
        
    - Character Tokens: ["T", "o", "k", "e", "n", "i", "z", "a", "t", "i", "o", "n", " ", "i", "s", " ", "i", "m", "p", "o", "r", "t", "a", "n", "t", "!"][[7](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ5XrzC21b_Ima0ob00QZGCvc_H3-MTydUhZ2IV4DhnSsgSdM_8gb4bVLfxww3EICZXf0Fx0nrK47Zo3SMJA4eB0-XPsF_q5Khni0X3WsJv_0m8dN-39_ClOIX7NaWCelbrbXi20KQDrd6Vb5WbcKNFjM-7GK9UmGA5ubXZR_B9xu2la2NIKrJVZsUZrA%3D%3D)].
        
    - Numerical Representation (example IDs): [2TokenID, 3izationID, 4isID, 5importantID, 6!ID][[6](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKuAnGymumgOocypBSZvqdxtv8s_m8ZyXGntPcwvNLvcIrXxnBbTgAJsPG9dz94jfty8Ix-3tEyVma_LSPZgzFkYNAtKgY27bwY5FsxnWRtocwmvfspaC6MUbX-gxZ-6fTh55erxMg4w5JimovIdfbBikjwcAVT085hApDS1LmU8Q%3D%3D)].
        

In summary, tokens are the basic units LLMs use to process text. Tokenization converts text into these units (often subwords), which are then mapped to numbers for the model[[4](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAI5hMEo13Rb50ZmUODC5MzEYm1tgVdz5bqJzVbviTcDYsIqlVHOfRxSI89-DKlYlC4colIFNxIKcc4qMRysfN7yCyUZGbVG3d_xlunQ3KgZiQZcoVLjB9WMUHNO2cpkb-ld5WhijRujM4Fb)][[9](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ433CD791L7XBqwf-kcR4dxxaeBScCDBn82GIv1XOhyxXy6MSIS211I60AD2sKEtPvqX26DtkEVvITqVysvNuNhjM2nlM1Nd2gVBqLb6bg14RtkeHkQgMevZuexn21yrrnbtRVMFBJ4F3PUXeNtWul8i4XEw%3D%3D)]. This process is fundamental to how models understand language, handle diverse inputs, and manage computational resources, directly impacting context limits, cost, and overall performance[[7](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ5XrzC21b_Ima0ob00QZGCvc_H3-MTydUhZ2IV4DhnSsgSdM_8gb4bVLfxww3EICZXf0Fx0nrK47Zo3SMJA4eB0-XPsF_q5Khni0X3WsJv_0m8dN-39_ClOIX7NaWCelbrbXi20KQDrd6Vb5WbcKNFjM-7GK9UmGA5ubXZR_B9xu2la2NIKrJVZsUZrA%3D%3D)][[16](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKpuZ7UG65VgbFJiEn3oMxkehcB4Mfhupk5SIicepT920oVaTVt8vgTLyQXZMZavKi0ePGmL8WzFyFb1uJRX4Arn7Ngz3QkiUtd34xJDUIVeVydo2cQbCcNJLKWj20Mbl3VQ6mhvu3nayh96FPdeiu75gYjFfKd-0KNiXZQY9L_TUpEzA%3D%3D)][[17](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKHxMTXeQS0HRHVoTYOOjaoCPuZbnBN0rXEci4GvFH1cNzolhWhDhL9CNctZqeU2yXvab8vDunstz9ReI3hTZcP_uq-nUx_5z7o3fb_Ij04KEt-TVBJSrdo5mrGqY9TpTunLQ%3D%3D)].

Search Sourceshelp

1. [substack.com](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKC9YCYUdgQ9yAKUElZM9lSbg5jo_kuS3NbOwqwv-qSM8Vdw9eeenqzAY6TyV_YvEDd87XptowuB0LaUTzV7bZ5eAQzlw8rCNYg86kMM6q2lSiazstelrjm270p9detG4gTbvRAt41S7CaPnqYwKPbHouPwHNHSOInvfW8%3D)
2. [christophergs.com](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALHQ0UtaD0Bg4VJcV6tK62NWr-E8l-iUvqOBFHglMWAM19S-UOxitFdpbBi4Y28dUA4mB1kn-Y37S8MVKxiqsCTw7eYgw5I9aFrEatNrSzhUWxkskjCJyXGdNMHOksdrfuTGpMWaMGR6DPxw0JMG2E1NkQ%3D)
3. [kelvin.legal](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ8RbvbWgs5gs6G3-zif5M_m3Wc_Qo3Sfgbkg0BOBVOPpoxfu-8CcOMUrlgYRsm6nwl5SQn-99lvhwx0c_XyQ283NRXbuYRPWJFilM2c78LrgJ7zI-H641hfNE1wHSC8yz92MP6uBuEbObyoqRqiNqwHQXnZ1isitVWK9qfOP0Kjuqu)
4. [huggingface.co](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAI5hMEo13Rb50ZmUODC5MzEYm1tgVdz5bqJzVbviTcDYsIqlVHOfRxSI89-DKlYlC4colIFNxIKcc4qMRysfN7yCyUZGbVG3d_xlunQ3KgZiQZcoVLjB9WMUHNO2cpkb-ld5WhijRujM4Fb)
5. [mistral.ai](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKr-l8HI8_LusObg8KlVBLRgdLDURGTsPs0qFZsgNCUQtcaVMkKTTeQtlb9TludjaL13mJZiVmIt7fkGeADZpvhwtct3wirs9_CV47rdFqbIwELHU5RSYbUEvIgpOFw0qKC)
6. [microsoft.com](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKuAnGymumgOocypBSZvqdxtv8s_m8ZyXGntPcwvNLvcIrXxnBbTgAJsPG9dz94jfty8Ix-3tEyVma_LSPZgzFkYNAtKgY27bwY5FsxnWRtocwmvfspaC6MUbX-gxZ-6fTh55erxMg4w5JimovIdfbBikjwcAVT085hApDS1LmU8Q%3D%3D)
7. [systenics.ai](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ5XrzC21b_Ima0ob00QZGCvc_H3-MTydUhZ2IV4DhnSsgSdM_8gb4bVLfxww3EICZXf0Fx0nrK47Zo3SMJA4eB0-XPsF_q5Khni0X3WsJv_0m8dN-39_ClOIX7NaWCelbrbXi20KQDrd6Vb5WbcKNFjM-7GK9UmGA5ubXZR_B9xu2la2NIKrJVZsUZrA%3D%3D)
8. [nvidia.com](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKIicIayDyxq5MGRpl8cYde5FzbY_39DEGaaWag6QSH9P5ZAmzpQkN77FzSYuA95panhiQNLXVzSKr1LGzXuZrGtoXrU4651rsp3-qNSzOTrQJ0ED5R9thhg3whYpVYBzpMeciQJG0Z)
9. [airbyte.com](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ433CD791L7XBqwf-kcR4dxxaeBScCDBn82GIv1XOhyxXy6MSIS211I60AD2sKEtPvqX26DtkEVvITqVysvNuNhjM2nlM1Nd2gVBqLb6bg14RtkeHkQgMevZuexn21yrrnbtRVMFBJ4F3PUXeNtWul8i4XEw%3D%3D)
10. [dzone.com](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAJ5bC7c8r_aul4Qln9EWZFYoFeDlRrpAhopmlx9KmKgKH6bg62cPs8HG6ms2YlD6JDlAbAs__hLYs_Bu5SsA0sWamIL5g2KcWpwAP1tx5cRUZCbK_UbzShd43DjEcDIM3_v563i9kizSVX8_E_uq_6e)
11. [towardsdatascience.com](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALes-FzTNwvqWW8g5keFYUF3E9u4uLDESZtaWi63_nEj5_TJVInyiBHekjRX0atg4B2XwtPE6VHrLj-dNFMfT8p_6Ui-YNhZ4xykILl41xaBCknMwMHEH5qxUjAUNkgVbmpXVY2Ah-6IDN-pWYcS2fm0RcSxvJN04WohyWMX7cOCAMdoCyqwsibQ4Q9nBI%3D)
12. [aman.ai](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAIJn1406nFi_hinn5mVMsvSH5WGlQ0O0wsj9twsdS2MFTb163wfALSRtDQI9X0kX8Vyy9EyDWC85fevBKFqPiZ8FwnxViZuZEBsh691qu-8llV3Vo8ZUbvKLeo%3D)
13. [ingoampt.com](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALUF9n7ICff6mRnjQjGaCQOTrq9X9IOsrHvF9JcNsmom8aVC-V63sE4rAsa9-xhhcresRhnrVfiGwrmkd-QtMKpcai7puI-vYm1zEy7n9FOmDccHA1KBZ2fK4x3Gld5S4yjPzd6zKwNJosLkuLNwk0JucsfSmE%3D)
14. [doptsw.com](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAK9CmfKiLEMWkCSy4i_S-GrS1wO8jJyGG6KPK18LoKI2YRRxBBXuLscYdRWJ8HZFNRgkGgTFvPylat5xJbYpEhUDlHdON41iLT-_EPB67CG9L7Nb_tQiTZr9H__84GpfzhgCTgF9f-Bpg%3D%3D)
15. [mdpi.com](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKvH8rMIeILRSUr5ufjLeZ4x5JfkVspElNCtH7s8bhhAPkeTi2uk9DyJYQcrlZkolwwS4WLuujRVEc3GIzffA6yN88DNq41K-ieHet1dukx2t8BhhKVeJaT3apyJk7q)
16. [edenai.co](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKpuZ7UG65VgbFJiEn3oMxkehcB4Mfhupk5SIicepT920oVaTVt8vgTLyQXZMZavKi0ePGmL8WzFyFb1uJRX4Arn7Ngz3QkiUtd34xJDUIVeVydo2cQbCcNJLKWj20Mbl3VQ6mhvu3nayh96FPdeiu75gYjFfKd-0KNiXZQY9L_TUpEzA%3D%3D)
17. [unstract.com](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKHxMTXeQS0HRHVoTYOOjaoCPuZbnBN0rXEci4GvFH1cNzolhWhDhL9CNctZqeU2yXvab8vDunstz9ReI3hTZcP_uq-nUx_5z7o3fb_Ij04KEt-TVBJSrdo5mrGqY9TpTunLQ%3D%3D)
18. [dabase.com](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqAKAuhn_c1c8ETXuLOrNe-yGhaDrZ_8aUeyEmaygDlXT1BZmfn7NYJDhUAwY_iEFSsgMRfRIYyzXfF8Le9ZtRNcNozK7crplSInymKO1D_TPqg94jgKk-h0JjPWZN-nGk9P8)
19. [tokenomics.ie](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALQJ71nvi7Yu281Xe4-CS6O1vjLWd08Kix4FVLEGiAhE05H--vxAMvG6AoyBv4E6_bicKdR1Ul8Eeeymty0ZQEWrCVWMbHq4w6kYjp-BBFwbmxsNrAsUF-FEiu-OWT6HFCeUqXevixDpo5JCxw3VrA12Vkg2BtzP7xpsbJq0zVAHXVAZxdF1hA8CEY%3D)
20. [dezlearn.com](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAWQVqALP7xmZCVCnEmS0knz2WtJp1Sm8lclSpsRgNW1kZgTn0hdcbAmpK1JusaAMZ2Q45IN_8yLzL_mL8aCDT_wTe3Yi7CF4ZoQjciECW27GUsM6ZUyrheqQJuv2che6kn_QTllhpaCEpe53zPPgjgHKwbULbkjlBZo%3D)